{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A single layer perceptron is the simplest neural network model that can be created. It simply consisits of a singular layer that takes n inputs and n randomly decided weights that it uses to calculate the weighted sum. \n",
    "2. This sum is passed through a activation function such as the sigmoid or reLu functions. The output produced from the activation fucntion is a binary one - that is either 1 or 0. \n",
    "3. After this, the final output is compared with the actual output given. The difference between the two is multiplied by the learning rate and the input and added to the weight as correction. This is essentially where the learning part of the algorithm takes place. \n",
    "4. They work well for simple and linearly seperable datasets. However, it was found that single-layer perceptrons do not work well with non linear data - that is data that is not linearly spearable, which makes it inefficient for large scale tasks such as image processing and even complex NLP tasks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation is an extremely important concept when it comes to neural networks. It forms the basis of all the learning that takes place in a neural network. Backpropogation finds its basis in calculus- more particularly the chain rule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take the following example:\n",
    "\n",
    "$$ y= \\sin (e^{x^2+x^4})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to differentiate y, then we get the following equation (1):\n",
    "\n",
    "$$ \\frac{dy}{dx}= \\frac{d(\\sin(e^{x^2+x^4}))}{dx} \\hspace{5 mm} $$\n",
    "$$ \\hspace{24 mm}= \\frac{d(\\sin t)}{dt}*\\frac{dt}{dx} \\hspace{10 mm} t= e^{x^2 + x^4} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (2):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{dt}{dx}= \\frac{d(e^{x^2+x^4})}{dx} $$\n",
    "$$ \\hspace{37 mm} = \\frac{e^u}{du}*\\frac{du}{dx} \\hspace{10 mm}  u= x^2 + x^4$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (3):\n",
    "\n",
    "$$ \\frac{du}{dx}= \\frac{d(x^2+x^4)}{dx} $$\n",
    "$$ \\hspace{7 mm} = 2x + 4x^3$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform backpropogation. We substitute results of Eq 3 in Eq 2, then the results of Eq 2 in Eq 1 to get our final output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (2):\n",
    "\n",
    "$$ \\frac{dt}{dx}= \\frac{d(e^u)}{du}*\\frac{du}{dx}= e^u * (2x + 4x^3) = e^{x^2 + x^4} * (2x + 4x^3) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (1):\n",
    "\n",
    "$$ \\frac{dy}{dx}=\\frac{d(\\sin t)}{dt}*\\frac{dt}{dx}= \\cos t *  e^{x^2 + x^4} * (2x + 4x^3) $$\n",
    "$$\\hspace{37mm}= \\cos e^{x^2 + x^4}  *  e^{x^2 + x^4} * (2x + 4x^3)  $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we solve the above problem with the help of backpropogation or the chain rule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation is a integral part of all neural networks. It is through backpropogation that the network learns from its mistakes and rectifies its errors to become better. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer perceptron is a combination of multiple layers of perceptrons weaved together.   \n",
    "It uses the outputs of the first layer as inputs of the next layer until finally after a particular number of layers, it reaches the output layer.   \n",
    "The layers in between the input and output layers are called hidden layers. As with the perceptron, MLP also has weights to be adjusted to train the system.   \n",
    "These weights now come in a matrix form at every junction between layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward:\n",
    "The first part of creating an MLP is developing the feedforward algorithm. Feedforward is essentially the process used to turn the input into an output. However, it is not as simple as in the perceptron, but now needs to iterated over the various number of layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropogation in MLP:\n",
    "Backpropagation relies primarily on the chain rule. We want to find out how changing the weights in a particular neuron affects the pre-defined cost function. Since we can indirectly relate the weights to the cost function through other variables, we can differentiate the cost function with respect to the weights by differentiating with the chain rule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the gradient $\\nabla C$ using the following equation:\n",
    "$$ \\nabla C = \\langle \\frac{\\delta C}{\\delta w_0},..., \\frac{\\delta C}{\\delta w_n} \\rangle$$  \n",
    "where, {$ w_0, w_1, w_2,...,w_n $} are the weights "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know $\\nabla C$, we can find change in weights:\n",
    "\n",
    "$$ \\Delta w = - \\eta \\nabla C  $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where, $\\eta $ is the learning parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
