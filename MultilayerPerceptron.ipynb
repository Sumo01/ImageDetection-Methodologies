{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reference:  \n",
    "><a href=https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141> Multilayer Perceptron: The beginnings of artificial neural networks </a>  \n",
    "> <a href=https://medium.com/engineer-quant/multilayer-perceptron-4453615c4337>Understanding the basics of MLPs</a>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A single layer perceptron is the simplest neural network model that can be created. It simply consisits of a singular layer that takes n inputs and n randomly decided weights that it uses to calculate the weighted sum. \n",
    "2. This sum is passed through a activation function such as the sigmoid or reLu functions. The output produced from the activation fucntion is a binary one - that is either 1 or 0. \n",
    "3. After this, the final output is compared with the actual output given. The difference between the two is multiplied by the learning rate and the input and added to the weight as correction. This is essentially where the learning part of the algorithm takes place. \n",
    "4. They work well for simple and linearly seperable datasets. However, it was found that single-layer perceptrons do not work well with non linear data - that is data that is not linearly spearable, which makes it inefficient for large scale tasks such as image processing and even complex NLP tasks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation is an extremely important concept when it comes to neural networks. It forms the basis of all the learning that takes place in a neural network. Backpropogation finds its basis in calculus- more particularly the chain rule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take the following example:\n",
    "\n",
    "$$ y= \\sin (e^{x^2+x^4})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to differentiate y, then we get the following equation (1):\n",
    "\n",
    "$$ \\frac{dy}{dx}= \\frac{d(\\sin(e^{x^2+x^4}))}{dx} \\hspace{5 mm} $$\n",
    "$$ \\hspace{24 mm}= \\frac{d(\\sin t)}{dt}*\\frac{dt}{dx} \\hspace{10 mm} t= e^{x^2 + x^4} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (2):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{dt}{dx}= \\frac{d(e^{x^2+x^4})}{dx} $$\n",
    "$$ \\hspace{37 mm} = \\frac{e^u}{du}*\\frac{du}{dx} \\hspace{10 mm}  u= x^2 + x^4$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (3):\n",
    "\n",
    "$$ \\frac{du}{dx}= \\frac{d(x^2+x^4)}{dx} $$\n",
    "$$ \\hspace{7 mm} = 2x + 4x^3$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform backpropogation. We substitute results of Eq 3 in Eq 2, then the results of Eq 2 in Eq 1 to get our final output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (2):\n",
    "\n",
    "$$ \\frac{dt}{dx}= \\frac{d(e^u)}{du}*\\frac{du}{dx}= e^u * (2x + 4x^3) = e^{x^2 + x^4} * (2x + 4x^3) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (1):\n",
    "\n",
    "$$ \\frac{dy}{dx}=\\frac{d(\\sin t)}{dt}*\\frac{dt}{dx}= \\cos t *  e^{x^2 + x^4} * (2x + 4x^3) $$\n",
    "$$\\hspace{37mm}= \\cos e^{x^2 + x^4}  *  e^{x^2 + x^4} * (2x + 4x^3)  $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we solve the above problem with the help of backpropogation or the chain rule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropogation is a integral part of all neural networks. It is through backpropogation that the network learns from its mistakes and rectifies its errors to become better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
